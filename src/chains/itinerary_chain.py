from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from src.config.config import GROQ_API_KEY

# Initialize the LLM using Groq's LLaMA model with API key
llm = ChatGroq(
    groq_api_key=GROQ_API_KEY,
    model_name="llama-3.3-70b-versatile",  # Using Groq's high-performance LLaMA 3.3 model
    temperature=0.3,  # Lower temperature for more factual and consistent outputs
)

# Define a chat prompt template to instruct the LLM for itinerary generation
itinerary_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful travel assistant. Create a one-day trip itinerary for the city: {city} based on the user's interests: {interests}. "
            "Provide a concise, bulleted itinerary with places to visit, eat, and enjoy.",
        ),
        ("human", "Create an itinerary for my day trip."),
    ]
)


# Function to generate the itinerary using city and interests
def generate_itinerary(city: str, interests: list[str]) -> str:
    """
    Generates a travel itinerary for a given city based on user interests.

    Args:
        city (str): The name of the city for the itinerary.
        interests (list[str]): List of user's interests (e.g., museums, food, nature).

    Returns:
        str: Formatted travel itinerary as generated by the LLM.
    """
    # Format the prompt messages with user input
    messages = itinerary_prompt.format_messages(
        city=city,
        interests=", ".join(interests),  # Convert list to comma-separated string
    )

    # Invoke the LLM with the formatted prompt
    response = llm.invoke(messages)

    # Return the content of the LLM's response
    return response.content
